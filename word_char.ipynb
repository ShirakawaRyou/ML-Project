{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import resample\n",
        "from scipy.sparse import hstack\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "def preprocess_and_balance(data_path, target_count=10000):\n",
        "    data = pd.read_csv(data_path, sep='\\t', names=['id', 'language', 'text'], encoding='latin-1')\n",
        "    print(f\"Original number of records: {len(data)}\")\n",
        "\n",
        "    data.dropna(subset=['language', 'text'], inplace=True)\n",
        "    print(f\"Number of records after removing NaNs: {len(data)}\")\n",
        "\n",
        "    lang_counts = data['language'].value_counts()\n",
        "    sufficient_langs = lang_counts[lang_counts >= target_count].index\n",
        "    data = data[data['language'].isin(sufficient_langs)]\n",
        "\n",
        "    print(f\"Number of language classes (samples ≥ {target_count}): {len(sufficient_langs)}\")\n",
        "\n",
        "    balanced_list = []\n",
        "    for lang in data['language'].unique():\n",
        "        subset = data[data['language'] == lang]\n",
        "        resampled = resample(\n",
        "            subset,\n",
        "            replace=(len(subset) < target_count),\n",
        "            n_samples=target_count,\n",
        "            random_state=42\n",
        "        )\n",
        "        balanced_list.append(resampled)\n",
        "\n",
        "    balanced_data = pd.concat(balanced_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n✅ Each language class has {target_count} samples. Total samples: {len(balanced_data)}\")\n",
        "    print(balanced_data['language'].value_counts())\n",
        "\n",
        "    return balanced_data\n",
        "\n",
        "def train_model(balanced_data):\n",
        "    print(\"🚀 Starting model training (word + char n-gram + Logistic Regression)\")\n",
        "\n",
        "    word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), max_features=50000, max_df=0.95, min_df=2)\n",
        "    char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 6), max_features=50000, max_df=0.95, min_df=2)\n",
        "\n",
        "    X_word = word_vectorizer.fit_transform(balanced_data['text'])\n",
        "    X_char = char_vectorizer.fit_transform(balanced_data['text'])\n",
        "    X = hstack([X_word, X_char])\n",
        "    y = balanced_data['language']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto')\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(\"\\n📈 Model evaluation:\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    os.makedirs('./models', exist_ok=True)\n",
        "    joblib.dump(model, './models/language_model_wordchar.pkl')\n",
        "    joblib.dump((word_vectorizer, char_vectorizer), './models/vectorizer_wordchar.pkl')\n",
        "    print(\"✅ Model training completed and saved!\")\n",
        "\n",
        "    return model, (word_vectorizer, char_vectorizer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = preprocess_and_balance(\"sentences.csv\", target_count=10000)\n",
        "    train_model(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg2Jc3TVos9v",
        "outputId": "60764cd9-709b-4956-cd85-7cc5fb4e78a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "原始資料筆數: 12731979\n",
            "移除 NaN 後資料筆數: 12731962\n",
            "語言類別數（樣本數 ≥ 10000）: 67\n",
            "\n",
            "✅ 每類語言樣本固定為 10000，總樣本數: 670000\n",
            "language\n",
            "hun    10000\n",
            "slk    10000\n",
            "fin    10000\n",
            "srp    10000\n",
            "ind    10000\n",
            "       ...  \n",
            "tgl    10000\n",
            "ita    10000\n",
            "fra    10000\n",
            "mar    10000\n",
            "lfn    10000\n",
            "Name: count, Length: 67, dtype: int64\n",
            "🚀 開始訓練模型（word + char n-gram + Logistic Regression）\n",
            "訓練集大小: 536000，測試集大小: 134000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📈 模型評估結果：\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ara       0.99      1.00      0.99      2000\n",
            "         asm       0.99      0.98      0.99      2000\n",
            "         bel       0.97      0.98      0.98      2000\n",
            "         ben       0.99      0.99      0.99      2000\n",
            "         ber       0.83      0.80      0.82      2000\n",
            "         bul       0.93      0.93      0.93      2000\n",
            "         ces       0.97      0.95      0.96      2000\n",
            "         ckb       1.00      0.99      1.00      2000\n",
            "         cmn       0.98      0.99      0.98      2000\n",
            "         dan       0.90      0.90      0.90      2000\n",
            "         deu       0.99      0.99      0.99      2000\n",
            "         ell       1.00      1.00      1.00      2000\n",
            "         eng       0.99      0.99      0.99      2000\n",
            "         epo       0.98      0.98      0.98      2000\n",
            "         fin       0.99      0.99      0.99      2000\n",
            "         fra       0.99      0.99      0.99      2000\n",
            "         gos       0.97      0.97      0.97      2000\n",
            "         hau       0.99      1.00      0.99      2000\n",
            "         heb       1.00      1.00      1.00      2000\n",
            "         hin       0.99      0.98      0.99      2000\n",
            "         hun       0.99      1.00      0.99      2000\n",
            "         hye       1.00      1.00      1.00      2000\n",
            "         ido       0.95      0.98      0.97      2000\n",
            "         ile       0.95      0.96      0.96      2000\n",
            "         ina       0.97      0.95      0.96      2000\n",
            "         ind       0.99      0.99      0.99      2000\n",
            "         isl       1.00      0.99      0.99      2000\n",
            "         ita       0.97      0.98      0.98      2000\n",
            "         jbo       0.99      0.99      0.99      2000\n",
            "         jpn       1.00      1.00      1.00      2000\n",
            "         kab       0.82      0.82      0.82      2000\n",
            "         kmr       1.00      0.99      0.99      2000\n",
            "         kor       1.00      1.00      1.00      2000\n",
            "         lat       0.98      0.97      0.98      2000\n",
            "         lfn       0.95      0.97      0.96      2000\n",
            "         lit       0.99      0.99      0.99      2000\n",
            "         lvs       0.99      0.99      0.99      2000\n",
            "         mar       0.98      0.99      0.99      2000\n",
            "         mkd       0.90      0.93      0.92      2000\n",
            "         nds       0.99      0.99      0.99      2000\n",
            "         nld       0.97      0.97      0.97      2000\n",
            "         nnb       0.99      0.99      0.99      2000\n",
            "         nno       0.89      0.91      0.90      2000\n",
            "         nob       0.86      0.84      0.85      2000\n",
            "         oci       0.97      0.98      0.97      2000\n",
            "         pes       0.99      0.99      0.99      2000\n",
            "         pol       0.99      0.99      0.99      2000\n",
            "         por       0.99      0.98      0.99      2000\n",
            "         ron       1.00      0.98      0.99      2000\n",
            "         rus       0.97      0.95      0.96      2000\n",
            "         shi       0.97      0.99      0.98      2000\n",
            "         slk       0.94      0.97      0.95      2000\n",
            "         spa       0.98      0.97      0.97      2000\n",
            "         srp       0.95      0.92      0.93      2000\n",
            "         swc       0.98      0.99      0.98      2000\n",
            "         swe       0.97      0.97      0.97      2000\n",
            "         tat       0.99      0.98      0.98      2000\n",
            "         tgl       1.00      0.99      1.00      2000\n",
            "         tig       1.00      1.00      1.00      2000\n",
            "         tlh       0.99      0.99      0.99      2000\n",
            "         tok       1.00      1.00      1.00      2000\n",
            "         tur       0.99      1.00      0.99      2000\n",
            "         ukr       0.97      0.96      0.96      2000\n",
            "         vie       1.00      1.00      1.00      2000\n",
            "         yid       1.00      1.00      1.00      2000\n",
            "         yue       0.99      0.98      0.99      2000\n",
            "         zgh       1.00      1.00      1.00      2000\n",
            "\n",
            "    accuracy                           0.97    134000\n",
            "   macro avg       0.97      0.97      0.97    134000\n",
            "weighted avg       0.97      0.97      0.97    134000\n",
            "\n",
            "✅ 模型訓練完成並已儲存！\n"
          ]
        }
      ]
    }
  ]
}